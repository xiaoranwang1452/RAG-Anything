#!/usr/bin/env python3
"""
RAG-Anything ç®€å•ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•é…ç½®å’Œä½¿ç”¨ RAG-Anything å¤„ç†æ–‡æ¡£
"""

import asyncio
import os
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤º RAG-Anything çš„åŸºæœ¬ç”¨æ³•"""
    
    print("ğŸš€ RAG-Anything ç®€å•ç¤ºä¾‹å¼€å§‹...")
    
    # æ£€æŸ¥ API å¯†é’¥
    api_key = os.getenv("LLM_BINDING_API_KEY") or os.getenv("OPENAI_API_KEY")
    base_url = os.getenv("LLM_BINDING_HOST", "https://yinli.one/v1")
    
    if not api_key:
        print("âš ï¸  æœªæ‰¾åˆ° API å¯†é’¥")
        print("è¯·è®¾ç½®æ‚¨çš„ API å¯†é’¥ï¼š")
        print("export LLM_BINDING_API_KEY='your-api-key-here'")
        print("\næˆ–è€…ç¼–è¾‘ .env æ–‡ä»¶æ·»åŠ ï¼š")
        print("LLM_BINDING_API_KEY=your-api-key-here")
        return
    
    print("âœ… æ‰¾åˆ° API å¯†é’¥")
    
    # åˆ›å»ºé…ç½®
    config = RAGAnythingConfig(
        working_dir="./example_rag_storage",
        parser="mineru",
        parse_method="auto",
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )
    
    # å®šä¹‰ LLM æ¨¡å‹å‡½æ•°
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    
    # å®šä¹‰è§†è§‰æ¨¡å‹å‡½æ•°
    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt} if system_prompt else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    } if image_data else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)
    
    # å®šä¹‰åµŒå…¥å‡½æ•°
    embedding_func = EmbeddingFunc(
        embedding_dim=1536,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-ada-002",
            api_key=api_key,
            base_url=base_url,
        ),
    )
    
    # åˆå§‹åŒ– RAGAnything
    print("ğŸ”§ åˆå§‹åŒ– RAGAnything...")
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )
    
    print("âœ… RAGAnything åˆå§‹åŒ–å®Œæˆ")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç¤ºä¾‹æ–‡æ¡£
    example_docs = [
        "example_doc/2005.11401v4.pdf",
        "example_doc/CS55-1_Week6_TutorialPresentation.pdf",
        "example_doc/CS55-Agentic Multimodal RAG_An Intelligent Framework for Scientific Concept Discovery from Text and Visuals.pdf"
    ]
    
    available_docs = [doc for doc in example_docs if os.path.exists(doc)]
    
    if available_docs:
        print(f"\nğŸ“„ æ‰¾åˆ° {len(available_docs)} ä¸ªç¤ºä¾‹æ–‡æ¡£")
        
        # å¤„ç†ç¬¬ä¸€ä¸ªæ–‡æ¡£
        doc_path = available_docs[0]
        print(f"ğŸ”„ å¤„ç†æ–‡æ¡£: {doc_path}")
        
        try:
            # å¤„ç†æ–‡æ¡£
            await rag.process_document_complete(
                file_path=doc_path,
                output_dir="./example_output",
                parse_method="auto"
            )
            
            print("âœ… æ–‡æ¡£å¤„ç†å®Œæˆ")
            
            # è¿›è¡ŒæŸ¥è¯¢
            print("\nğŸ” è¿›è¡ŒæŸ¥è¯¢æµ‹è¯•...")
            result = await rag.aquery(
                "è¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                mode="hybrid"
            )
            
            print("ğŸ“ æŸ¥è¯¢ç»“æœ:")
            print(result)
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            print("è¿™å¯èƒ½æ˜¯ç”±äº API é™åˆ¶æˆ–ç½‘ç»œé—®é¢˜å¯¼è‡´çš„")
    
    else:
        print("\nğŸ“„ æœªæ‰¾åˆ°ç¤ºä¾‹æ–‡æ¡£")
        print("æ‚¨å¯ä»¥ï¼š")
        print("1. å°†æ‚¨çš„ PDF æ–‡æ¡£æ”¾åœ¨é¡¹ç›®ç›®å½•ä¸­")
        print("2. ä¿®æ”¹è„šæœ¬ä¸­çš„ file_path å˜é‡")
        print("3. è¿è¡Œè„šæœ¬å¤„ç†æ‚¨çš„æ–‡æ¡£")
    
    print("\nğŸ‰ ç¤ºä¾‹å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
