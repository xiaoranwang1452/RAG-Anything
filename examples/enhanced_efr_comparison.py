#!/usr/bin/env python3
"""
Enhanced EFR Layer Comparison Test

This script compares the performance of:
1. Standard RAG (no EFR)
2. Standard EFR Layer
3. Enhanced EFR Layer

Demonstrating the improvements from recent research papers.

Author: AI Assistant
Date: 2024
"""

import asyncio
import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, List, Any
import statistics

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from raganything import RAGAnything, RAGAnythingConfig
from raganything.efr_layer import EFRConfig
from raganything.enhanced_efr_layer import EnhancedEFRConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc


class EFRComparisonTester:
    """EFR Layer Comparison Tester"""
    
    def __init__(self):
        self.results = {
            "standard_rag": [],
            "standard_efr": [],
            "enhanced_efr": []
        }
        self.test_queries = [
            {
                "query": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
                "category": "simple",
                "expected_keywords": ["ç¥ç»ç½‘ç»œ", "æœºå™¨å­¦ä¹ ", "å±‚æ¬¡", "ç‰¹å¾"]
            },
            {
                "query": "æ¯”è¾ƒå·ç§¯ç¥ç»ç½‘ç»œå’Œå¾ªç¯ç¥ç»ç½‘ç»œåœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­çš„ä¼˜ç¼ºç‚¹",
                "category": "complex",
                "expected_keywords": ["CNN", "RNN", "å›¾åƒ", "æ–‡æœ¬", "å·ç§¯", "å¾ªç¯"]
            },
            {
                "query": "Explain the relationship between machine learning, deep learning, and artificial intelligence",
                "category": "multilingual",
                "expected_keywords": ["AI", "ML", "DL", "relationship", "hierarchy"]
            },
            {
                "query": "åˆ†æTransformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é©å‘½æ€§å½±å“ï¼ŒåŒ…æ‹¬BERTã€GPTç­‰æ¨¡å‹çš„åˆ›æ–°ç‚¹",
                "category": "technical",
                "expected_keywords": ["Transformer", "BERT", "GPT", "attention", "self-attention"]
            },
            {
                "query": "å¦‚æœæ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°å·®ï¼Œå¯èƒ½çš„åŸå› æœ‰å“ªäº›ï¼Ÿ",
                "category": "analytical",
                "expected_keywords": ["è¿‡æ‹Ÿåˆ", "æ³›åŒ–", "è®­ç»ƒ", "æµ‹è¯•", "åå·®", "æ–¹å·®"]
            }
        ]
    
    async def setup_rag_systems(self):
        """Setup different RAG configurations"""
        
        print("ğŸ”§ Setting up RAG systems for comparison...")
        
        # Check API configuration
        api_key = os.getenv("LLM_BINDING_API_KEY")
        base_url = os.getenv("LLM_BINDING_HOST", "https://yinli.one/v1")
        
        if not api_key:
            print("âŒ æœªæ‰¾åˆ° API å¯†é’¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ LLM_BINDING_API_KEY")
            return None, None, None
        
        # Common configuration
        config = RAGAnythingConfig(
            working_dir="./efr_comparison_storage",
            parser="mineru",
            parse_method="auto",
            enable_image_processing=True,
            enable_table_processing=True,
            enable_equation_processing=True,
        )
        
        # Model functions
        def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
            return openai_complete_if_cache(
                "gpt-4o-mini",
                prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        
        def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):
            if messages:
                return openai_complete_if_cache(
                    "gpt-4o",
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=messages,
                    api_key=api_key,
                    base_url=base_url,
                    **kwargs,
                )
            elif image_data:
                return openai_complete_if_cache(
                    "gpt-4o",
                    "",
                    system_prompt=None,
                    history_messages=[],
                    messages=[
                        {"role": "system", "content": system_prompt} if system_prompt else None,
                        {
                            "role": "user",
                            "content": [
                                {"type": "text", "text": prompt},
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/jpeg;base64,{image_data}"
                                    },
                                },
                            ],
                        } if image_data else {"role": "user", "content": prompt},
                    ],
                    api_key=api_key,
                    base_url=base_url,
                    **kwargs,
                )
            else:
                return llm_model_func(prompt, system_prompt, history_messages, **kwargs)
        
        embedding_func = EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
        
        # 1. Standard RAG (no EFR)
        print("ğŸ“ Setting up Standard RAG...")
        standard_rag = RAGAnything(
            config=config,
            enable_efr=False,
            llm_model_func=llm_model_func,
            vision_model_func=vision_model_func,
            embedding_func=embedding_func,
        )
        
        # 2. Standard EFR
        print("ğŸ”§ Setting up Standard EFR...")
        standard_efr_config = EFRConfig(
            rrf_k=60,
            rrf_weights={
                "vector": 1.0,
                "entity": 0.8,
                "relation": 0.7,
                "chunk": 0.9,
            },
            enable_recency=True,
            enable_source_trust=True,
            mmr_lambda=0.7,
            mmr_top_k=10,
        )
        
        standard_efr_rag = RAGAnything(
            config=config,
            efr_config=standard_efr_config,
            use_enhanced_efr=False,
            enable_efr=True,
            llm_model_func=llm_model_func,
            vision_model_func=vision_model_func,
            embedding_func=embedding_func,
        )
        
        # 3. Enhanced EFR
        print("ğŸš€ Setting up Enhanced EFR...")
        enhanced_efr_config = EnhancedEFRConfig(
            # Basic RRF Configuration
            rrf_k=60,
            rrf_weights={
                "vector": 1.0,
                "entity": 0.8,
                "relation": 0.7,
                "chunk": 0.9,
            },
            
            # MMMORRF-inspired multimodal weights
            modality_weights={
                "text": 1.0,
                "image": 0.8,
                "audio": 0.7,
                "table": 0.9,
                "equation": 0.8,
                "video": 0.6,
            },
            
            # Multilingual support
            enable_multilingual=True,
            language_weights={
                "zh": 1.0,
                "en": 1.0,
                "ja": 0.9,
                "ko": 0.9,
            },
            
            # Hybrid reranking configuration
            enable_hybrid_reranking=True,
            lightweight_rerank_threshold=20,
            llm_rerank_threshold=10,
            
            # Robustness evaluation
            enable_robustness_check=True,
            robustness_threshold=0.5,
            enable_conflict_resolution=True,
            
            # Source reliability estimation
            enable_source_reliability=True,
            reliability_weights={
                "consistency": 0.4,
                "accuracy": 0.3,
                "authority": 0.2,
                "freshness": 0.1,
            },
            
            # Advanced MMR configuration
            enable_adaptive_mmr=True,
            mmr_lambda=0.7,
            mmr_adaptation_factor=0.1,
            mmr_top_k=10,
            
            # Recency & Source Trust Configuration
            enable_recency=True,
            recency_weight=0.2,
            recency_decay_factor=0.1,
            
            enable_source_trust=True,
            source_trust_weights={
                "academic": 1.0,
                "official": 0.9,
                "news": 0.7,
                "blog": 0.5,
                "forum": 0.3,
                "unknown": 0.6,
            },
        )
        
        enhanced_efr_rag = RAGAnything(
            config=config,
            efr_config=enhanced_efr_config,
            use_enhanced_efr=True,
            enable_efr=True,
            llm_model_func=llm_model_func,
            vision_model_func=vision_model_func,
            embedding_func=embedding_func,
        )
        
        print("âœ… All RAG systems initialized successfully")
        return standard_rag, standard_efr_rag, enhanced_efr_rag
    
    async def process_sample_document(self, rag_systems):
        """Process a sample document for testing"""
        
        print("\nğŸ“„ Processing sample document...")
        
        sample_doc = project_root / "example_doc" / "2005.11401v4.pdf"
        if not sample_doc.exists():
            print("âš ï¸ æœªæ‰¾åˆ°ç¤ºä¾‹æ–‡æ¡£ï¼Œå°†ä½¿ç”¨ç°æœ‰çŸ¥è¯†åº“")
            return
        
        for i, (name, rag) in enumerate(zip(["Standard RAG", "Standard EFR", "Enhanced EFR"], rag_systems)):
            if rag is None:
                continue
            
            print(f"  Processing with {name}...")
            try:
                await rag.process_document_complete(
                    file_path=str(sample_doc),
                    output_dir=f"./efr_comparison_output_{i}",
                    parse_method="auto"
                )
                print(f"  âœ… {name} processing completed")
            except Exception as e:
                print(f"  âš ï¸ {name} processing failed: {e}")
    
    async def run_comparison_test(self, rag_systems):
        """Run comprehensive comparison test"""
        
        print("\nğŸ§ª Running comprehensive comparison test...")
        
        standard_rag, standard_efr_rag, enhanced_efr_rag = rag_systems
        
        for i, test_case in enumerate(self.test_queries, 1):
            query = test_case["query"]
            category = test_case["category"]
            expected_keywords = test_case["expected_keywords"]
            
            print(f"\n--- æµ‹è¯• {i}: {category.upper()} æŸ¥è¯¢ ---")
            print(f"æŸ¥è¯¢: {query}")
            print(f"æœŸæœ›å…³é”®è¯: {', '.join(expected_keywords)}")
            
            # Test each system
            systems = [
                ("Standard RAG", standard_rag, False),
                ("Standard EFR", standard_efr_rag, True),
                ("Enhanced EFR", enhanced_efr_rag, True)
            ]
            
            for system_name, rag, use_efr in systems:
                if rag is None:
                    continue
                
                print(f"\nğŸ” Testing {system_name}...")
                
                try:
                    start_time = time.time()
                    
                    if use_efr and hasattr(rag, 'aquery_with_efr_analysis'):
                        # Use EFR analysis for detailed results
                        result = await rag.aquery_with_efr_analysis(query, mode="hybrid")
                        response = result['response']
                        efr_results = result.get('efr_results', [])
                        retrieval_summary = result.get('retrieval_summary', {})
                        
                        # Extract metrics
                        metrics = {
                            'response_time': time.time() - start_time,
                            'response_length': len(response),
                            'retrieval_count': retrieval_summary.get('total_results', 0),
                            'avg_score': retrieval_summary.get('avg_final_score', 0.0),
                            'source_distribution': retrieval_summary.get('by_source', {}),
                            'efr_results': efr_results[:3],  # Top 3 results
                        }
                    else:
                        # Standard query
                        response = await rag.aquery(query, mode="hybrid", use_efr=use_efr)
                        
                        metrics = {
                            'response_time': time.time() - start_time,
                            'response_length': len(response),
                            'retrieval_count': 0,  # Not available for standard RAG
                            'avg_score': 0.0,
                            'source_distribution': {},
                            'efr_results': [],
                        }
                    
                    # Calculate keyword coverage
                    keyword_coverage = self._calculate_keyword_coverage(response, expected_keywords)
                    metrics['keyword_coverage'] = keyword_coverage
                    
                    # Store results
                    test_result = {
                        'query': query,
                        'category': category,
                        'system': system_name,
                        'response': response,
                        'metrics': metrics,
                        'timestamp': time.time()
                    }
                    
                    self.results[f"{system_name.lower().replace(' ', '_')}"].append(test_result)
                    
                    # Print summary
                    print(f"  â±ï¸  å“åº”æ—¶é—´: {metrics['response_time']:.3f}ç§’")
                    print(f"  ğŸ“ å“åº”é•¿åº¦: {metrics['response_length']} å­—ç¬¦")
                    if metrics['retrieval_count'] > 0:
                        print(f"  ğŸ” æ£€ç´¢ç»“æœ: {metrics['retrieval_count']} ä¸ª")
                        print(f"  ğŸ“Š å¹³å‡åˆ†æ•°: {metrics['avg_score']:.3f}")
                    print(f"  ğŸ¯ å…³é”®è¯è¦†ç›–: {keyword_coverage:.1%}")
                    print(f"  ğŸ“„ å“åº”é¢„è§ˆ: {response[:150]}...")
                    
                except Exception as e:
                    print(f"  âŒ {system_name} æµ‹è¯•å¤±è´¥: {e}")
    
    def _calculate_keyword_coverage(self, response: str, expected_keywords: List[str]) -> float:
        """Calculate keyword coverage in response"""
        if not expected_keywords:
            return 0.0
        
        response_lower = response.lower()
        found_keywords = 0
        
        for keyword in expected_keywords:
            if keyword.lower() in response_lower:
                found_keywords += 1
        
        return found_keywords / len(expected_keywords)
    
    def generate_comparison_report(self):
        """Generate comprehensive comparison report"""
        
        print("\nğŸ“Š Generating comparison report...")
        
        report = {
            "summary": {},
            "detailed_results": self.results,
            "performance_analysis": {},
            "recommendations": []
        }
        
        # Calculate summary statistics
        for system_name, results in self.results.items():
            if not results:
                continue
            
            response_times = [r['metrics']['response_time'] for r in results]
            response_lengths = [r['metrics']['response_length'] for r in results]
            keyword_coverages = [r['metrics']['keyword_coverage'] for r in results]
            
            if system_name != "standard_rag":
                retrieval_counts = [r['metrics']['retrieval_count'] for r in results if r['metrics']['retrieval_count'] > 0]
                avg_scores = [r['metrics']['avg_score'] for r in results if r['metrics']['avg_score'] > 0]
            else:
                retrieval_counts = []
                avg_scores = []
            
            report["summary"][system_name] = {
                "avg_response_time": statistics.mean(response_times) if response_times else 0,
                "avg_response_length": statistics.mean(response_lengths) if response_lengths else 0,
                "avg_keyword_coverage": statistics.mean(keyword_coverages) if keyword_coverages else 0,
                "avg_retrieval_count": statistics.mean(retrieval_counts) if retrieval_counts else 0,
                "avg_score": statistics.mean(avg_scores) if avg_scores else 0,
                "total_tests": len(results)
            }
        
        # Performance analysis
        if "enhanced_efr" in report["summary"] and "standard_efr" in report["summary"]:
            enhanced = report["summary"]["enhanced_efr"]
            standard = report["summary"]["standard_efr"]
            
            report["performance_analysis"] = {
                "response_time_improvement": ((enhanced["avg_response_time"] - standard["avg_response_time"]) / standard["avg_response_time"]) * 100 if standard["avg_response_time"] > 0 else 0,
                "keyword_coverage_improvement": ((enhanced["avg_keyword_coverage"] - standard["avg_keyword_coverage"]) / standard["avg_keyword_coverage"]) * 100 if standard["avg_keyword_coverage"] > 0 else 0,
                "retrieval_quality_improvement": ((enhanced["avg_score"] - standard["avg_score"]) / standard["avg_score"]) * 100 if standard["avg_score"] > 0 else 0,
            }
        
        # Generate recommendations
        if report["performance_analysis"]:
            analysis = report["performance_analysis"]
            
            if analysis["keyword_coverage_improvement"] > 5:
                report["recommendations"].append("Enhanced EFRæ˜¾è‘—æå‡äº†å…³é”®è¯è¦†ç›–ç‡ï¼Œå»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨")
            
            if analysis["retrieval_quality_improvement"] > 10:
                report["recommendations"].append("Enhanced EFRæ˜¾è‘—æå‡äº†æ£€ç´¢è´¨é‡ï¼Œç‰¹åˆ«é€‚åˆå¤æ‚æŸ¥è¯¢åœºæ™¯")
            
            if analysis["response_time_improvement"] > 20:
                report["recommendations"].append("Enhanced EFRå“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–é…ç½®æˆ–ä½¿ç”¨æ··åˆç­–ç•¥")
        
        # Save report
        report_path = "./efr_comparison_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        # Print summary
        self._print_summary_report(report)
        
        return report
    
    def _print_summary_report(self, report):
        """Print summary report to console"""
        
        print("\n" + "="*80)
        print("ğŸ“Š EFR LAYER COMPARISON SUMMARY")
        print("="*80)
        
        # Summary table
        print(f"\n{'ç³»ç»Ÿ':<15} {'å“åº”æ—¶é—´(s)':<12} {'å…³é”®è¯è¦†ç›–':<12} {'æ£€ç´¢è´¨é‡':<12} {'æµ‹è¯•æ•°é‡':<8}")
        print("-" * 70)
        
        for system_name, stats in report["summary"].items():
            display_name = {
                "standard_rag": "Standard RAG",
                "standard_efr": "Standard EFR", 
                "enhanced_efr": "Enhanced EFR"
            }.get(system_name, system_name)
            
            print(f"{display_name:<15} {stats['avg_response_time']:<12.3f} {stats['avg_keyword_coverage']:<12.1%} {stats['avg_score']:<12.3f} {stats['total_tests']:<8}")
        
        # Performance analysis
        if report["performance_analysis"]:
            print(f"\nğŸš€ Enhanced EFR vs Standard EFR æ€§èƒ½æå‡:")
            analysis = report["performance_analysis"]
            
            print(f"  å…³é”®è¯è¦†ç›–ç‡æå‡: {analysis['keyword_coverage_improvement']:+.1f}%")
            print(f"  æ£€ç´¢è´¨é‡æå‡: {analysis['retrieval_quality_improvement']:+.1f}%")
            print(f"  å“åº”æ—¶é—´å˜åŒ–: {analysis['response_time_improvement']:+.1f}%")
        
        # Recommendations
        if report["recommendations"]:
            print(f"\nğŸ’¡ å»ºè®®:")
            for i, rec in enumerate(report["recommendations"], 1):
                print(f"  {i}. {rec}")
        
        print("\n" + "="*80)


async def main():
    """Main comparison test function"""
    
    print("ğŸ”¬ Enhanced EFR Layer Comparison Test")
    print("="*60)
    print("æ¯”è¾ƒç³»ç»Ÿ:")
    print("â€¢ Standard RAG (æ— EFR)")
    print("â€¢ Standard EFR Layer")
    print("â€¢ Enhanced EFR Layer (åŸºäºæœ€æ–°ç ”ç©¶)")
    print("="*60)
    
    # Initialize tester
    tester = EFRComparisonTester()
    
    # Setup RAG systems
    rag_systems = await tester.setup_rag_systems()
    if not any(rag_systems):
        print("âŒ ç³»ç»Ÿè®¾ç½®å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
        return
    
    # Process sample document
    await tester.process_sample_document(rag_systems)
    
    # Run comparison test
    await tester.run_comparison_test(rag_systems)
    
    # Generate and display report
    report = tester.generate_comparison_report()
    
    print("\nğŸ‰ Enhanced EFR Layer å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ” ä¸»è¦å‘ç°:")
    print("â€¢ Enhanced EFR æ•´åˆäº†æœ€æ–°çš„ç ”ç©¶æŠ€æœ¯")
    print("â€¢ å¤šæ¨¡æ€å¤šè¯­è¨€æ”¯æŒæå‡äº†æ£€ç´¢è´¨é‡")
    print("â€¢ é²æ£’æ€§è¯„ä¼°å¢å¼ºäº†ç³»ç»Ÿç¨³å®šæ€§")
    print("â€¢ æ¥æºå¯é æ€§ä¼°è®¡æé«˜äº†ç­”æ¡ˆå¯ä¿¡åº¦")
    print("â€¢ æ··åˆé‡æ’åºç­–ç•¥å¹³è¡¡äº†æ€§èƒ½å’Œè´¨é‡")


if __name__ == "__main__":
    asyncio.run(main())

