#!/usr/bin/env python3
"""
EFR Layer æ•ˆæœå¯¹æ¯”æµ‹è¯•è„šæœ¬

è¿™ä¸ªè„šæœ¬æä¾›äº†å¤šç§æ–¹å¼æ¥å¯¹æ¯”EFRå±‚å¯ç”¨å‰åçš„æ•ˆæœï¼š
1. å¹¶æ’å¯¹æ¯”æ ‡å‡†æŸ¥è¯¢å’ŒEFRå¢å¼ºæŸ¥è¯¢
2. è¯¦ç»†çš„æ£€ç´¢ç»“æœåˆ†æå¯¹æ¯”
3. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
4. å¯è§†åŒ–å¯¹æ¯”æŠ¥å‘Š

Author: AI Assistant
Date: 2024
"""

import asyncio
import os
import sys
import time
import json
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass, asdict

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from raganything import RAGAnything, RAGAnythingConfig
from raganything.efr_layer import EFRConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc


@dataclass
class QueryResult:
    """æŸ¥è¯¢ç»“æœæ•°æ®ç»“æ„"""
    query: str
    response: str
    retrieval_count: int
    avg_score: float
    processing_time: float
    source_distribution: Dict[str, int]
    top_results: List[Dict[str, Any]]
    metadata: Dict[str, Any] = None


@dataclass
class ComparisonReport:
    """å¯¹æ¯”æŠ¥å‘Šæ•°æ®ç»“æ„"""
    query: str
    standard_result: QueryResult
    efr_result: QueryResult
    improvement_metrics: Dict[str, float]
    detailed_analysis: Dict[str, Any]


class EFRComparison:
    """EFRæ•ˆæœå¯¹æ¯”æµ‹è¯•ç±»"""
    
    def __init__(self, rag: RAGAnything):
        self.rag = rag
        self.comparison_results = []
    
    async def compare_single_query(
        self, 
        query: str, 
        mode: str = "hybrid",
        save_detailed: bool = True
    ) -> ComparisonReport:
        """
        å¯¹æ¯”å•ä¸ªæŸ¥è¯¢çš„EFRå‰åæ•ˆæœ
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            mode: æŸ¥è¯¢æ¨¡å¼
            save_detailed: æ˜¯å¦ä¿å­˜è¯¦ç»†åˆ†æ
            
        Returns:
            ComparisonReport: å¯¹æ¯”æŠ¥å‘Š
        """
        print(f"\nğŸ” å¯¹æ¯”æŸ¥è¯¢: {query}")
        print("=" * 80)
        
        # 1. æ ‡å‡†æŸ¥è¯¢ï¼ˆä¸ä½¿ç”¨EFRï¼‰
        print("ğŸ“ æ‰§è¡Œæ ‡å‡†æŸ¥è¯¢...")
        start_time = time.time()
        
        try:
            standard_response = await self.rag.aquery(query, mode=mode, use_efr=False)
            standard_time = time.time() - start_time
            
            # è·å–æ ‡å‡†æŸ¥è¯¢çš„æ£€ç´¢ä¿¡æ¯ï¼ˆé€šè¿‡ä¸Šä¸‹æ–‡åˆ†æï¼‰
            standard_analysis = await self._analyze_standard_query(query, mode)
            
            standard_result = QueryResult(
                query=query,
                response=standard_response,
                retrieval_count=standard_analysis.get('retrieval_count', 0),
                avg_score=standard_analysis.get('avg_score', 0.0),
                processing_time=standard_time,
                source_distribution=standard_analysis.get('source_distribution', {}),
                top_results=standard_analysis.get('top_results', []),
                metadata=standard_analysis
            )
            
        except Exception as e:
            print(f"âŒ æ ‡å‡†æŸ¥è¯¢å¤±è´¥: {e}")
            standard_result = QueryResult(
                query=query,
                response=f"æŸ¥è¯¢å¤±è´¥: {e}",
                retrieval_count=0,
                avg_score=0.0,
                processing_time=0.0,
                source_distribution={},
                top_results=[],
                metadata={"error": str(e)}
            )
        
        # 2. EFRå¢å¼ºæŸ¥è¯¢
        print("ğŸš€ æ‰§è¡ŒEFRå¢å¼ºæŸ¥è¯¢...")
        start_time = time.time()
        
        try:
            efr_analysis = await self.rag.aquery_with_efr_analysis(query, mode=mode)
            efr_time = time.time() - start_time
            
            efr_result = QueryResult(
                query=query,
                response=efr_analysis['response'],
                retrieval_count=efr_analysis['retrieval_summary']['total_results'],
                avg_score=efr_analysis['retrieval_summary']['avg_final_score'],
                processing_time=efr_time,
                source_distribution=efr_analysis['retrieval_summary']['by_source'],
                top_results=efr_analysis['efr_results'][:5],  # å‰5ä¸ªç»“æœ
                metadata=efr_analysis
            )
            
        except Exception as e:
            print(f"âŒ EFRæŸ¥è¯¢å¤±è´¥: {e}")
            efr_result = QueryResult(
                query=query,
                response=f"EFRæŸ¥è¯¢å¤±è´¥: {e}",
                retrieval_count=0,
                avg_score=0.0,
                processing_time=0.0,
                source_distribution={},
                top_results=[],
                metadata={"error": str(e)}
            )
        
        # 3. è®¡ç®—æ”¹è¿›æŒ‡æ ‡
        improvement_metrics = self._calculate_improvement_metrics(standard_result, efr_result)
        
        # 4. ç”Ÿæˆè¯¦ç»†åˆ†æ
        detailed_analysis = self._generate_detailed_analysis(standard_result, efr_result)
        
        # 5. åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š
        report = ComparisonReport(
            query=query,
            standard_result=standard_result,
            efr_result=efr_result,
            improvement_metrics=improvement_metrics,
            detailed_analysis=detailed_analysis
        )
        
        # 6. æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
        self._display_comparison(report)
        
        # 7. ä¿å­˜è¯¦ç»†ç»“æœ
        if save_detailed:
            await self._save_detailed_comparison(report)
        
        self.comparison_results.append(report)
        return report
    
    async def _analyze_standard_query(self, query: str, mode: str) -> Dict[str, Any]:
        """åˆ†ææ ‡å‡†æŸ¥è¯¢çš„æ£€ç´¢ä¿¡æ¯"""
        try:
            # è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
            context = await self.rag.lightrag.aquery(
                query, 
                param=self.rag.lightrag.QueryParam(mode=mode, only_need_context=True)
            )
            
            # ç®€å•åˆ†æä¸Šä¸‹æ–‡
            lines = context.split('\n')
            retrieval_count = len([line for line in lines if line.strip() and len(line.strip()) > 20])
            
            # ä¼°ç®—åˆ†æ•°ï¼ˆåŸºäºä¸Šä¸‹æ–‡é•¿åº¦å’Œè´¨é‡ï¼‰
            avg_score = min(1.0, len(context) / 1000)  # ç®€å•çš„å¯å‘å¼è¯„åˆ†
            
            return {
                'retrieval_count': retrieval_count,
                'avg_score': avg_score,
                'source_distribution': {'context': retrieval_count},
                'top_results': [{'content': line[:100], 'score': avg_score} for line in lines[:3] if line.strip()],
                'context_length': len(context)
            }
            
        except Exception as e:
            return {
                'retrieval_count': 0,
                'avg_score': 0.0,
                'source_distribution': {},
                'top_results': [],
                'error': str(e)
            }
    
    def _calculate_improvement_metrics(
        self, 
        standard: QueryResult, 
        efr: QueryResult
    ) -> Dict[str, float]:
        """è®¡ç®—æ”¹è¿›æŒ‡æ ‡"""
        metrics = {}
        
        # æ£€ç´¢æ•°é‡æ”¹è¿›
        if standard.retrieval_count > 0:
            metrics['retrieval_count_improvement'] = (
                (efr.retrieval_count - standard.retrieval_count) / standard.retrieval_count * 100
            )
        else:
            metrics['retrieval_count_improvement'] = 100.0 if efr.retrieval_count > 0 else 0.0
        
        # å¹³å‡åˆ†æ•°æ”¹è¿›
        if standard.avg_score > 0:
            metrics['score_improvement'] = (
                (efr.avg_score - standard.avg_score) / standard.avg_score * 100
            )
        else:
            metrics['score_improvement'] = 100.0 if efr.avg_score > 0 else 0.0
        
        # å¤„ç†æ—¶é—´å˜åŒ–
        if standard.processing_time > 0:
            metrics['time_change'] = (
                (efr.processing_time - standard.processing_time) / standard.processing_time * 100
            )
        else:
            metrics['time_change'] = 0.0
        
        # å“åº”è´¨é‡æ”¹è¿›ï¼ˆåŸºäºé•¿åº¦å’Œç»“æ„ï¼‰
        standard_quality = len(standard.response) / 1000  # ç®€å•å¯å‘å¼
        efr_quality = len(efr.response) / 1000
        if standard_quality > 0:
            metrics['response_quality_improvement'] = (
                (efr_quality - standard_quality) / standard_quality * 100
            )
        else:
            metrics['response_quality_improvement'] = 100.0 if efr_quality > 0 else 0.0
        
        return metrics
    
    def _generate_detailed_analysis(
        self, 
        standard: QueryResult, 
        efr: QueryResult
    ) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†åˆ†æ"""
        analysis = {
            'response_comparison': {
                'standard_length': len(standard.response),
                'efr_length': len(efr.response),
                'length_difference': len(efr.response) - len(standard.response),
                'standard_preview': standard.response[:200] + "..." if len(standard.response) > 200 else standard.response,
                'efr_preview': efr.response[:200] + "..." if len(efr.response) > 200 else efr.response,
            },
            'retrieval_analysis': {
                'standard_sources': standard.source_distribution,
                'efr_sources': efr.source_distribution,
                'source_diversity': len(efr.source_distribution) - len(standard.source_distribution),
            },
            'performance_analysis': {
                'standard_time': standard.processing_time,
                'efr_time': efr.processing_time,
                'time_overhead': efr.processing_time - standard.processing_time,
                'time_overhead_percent': ((efr.processing_time - standard.processing_time) / standard.processing_time * 100) if standard.processing_time > 0 else 0,
            },
            'quality_indicators': {
                'standard_avg_score': standard.avg_score,
                'efr_avg_score': efr.avg_score,
                'score_improvement': efr.avg_score - standard.avg_score,
                'standard_top_results': len(standard.top_results),
                'efr_top_results': len(efr.top_results),
            }
        }
        
        return analysis
    
    def _display_comparison(self, report: ComparisonReport):
        """æ˜¾ç¤ºå¯¹æ¯”ç»“æœ"""
        print(f"\nğŸ“Š å¯¹æ¯”ç»“æœ - æŸ¥è¯¢: {report.query}")
        print("=" * 80)
        
        # åŸºæœ¬æŒ‡æ ‡å¯¹æ¯”
        print("\nğŸ“ˆ åŸºæœ¬æŒ‡æ ‡å¯¹æ¯”:")
        print(f"{'æŒ‡æ ‡':<25} {'æ ‡å‡†æŸ¥è¯¢':<15} {'EFRæŸ¥è¯¢':<15} {'æ”¹è¿›':<15}")
        print("-" * 70)
        print(f"{'æ£€ç´¢ç»“æœæ•°é‡':<25} {report.standard_result.retrieval_count:<15} {report.efr_result.retrieval_count:<15} {report.improvement_metrics.get('retrieval_count_improvement', 0):+.1f}%")
        print(f"{'å¹³å‡åˆ†æ•°':<25} {report.standard_result.avg_score:<15.3f} {report.efr_result.avg_score:<15.3f} {report.improvement_metrics.get('score_improvement', 0):+.1f}%")
        print(f"{'å¤„ç†æ—¶é—´(ç§’)':<25} {report.standard_result.processing_time:<15.3f} {report.efr_result.processing_time:<15.3f} {report.improvement_metrics.get('time_change', 0):+.1f}%")
        print(f"{'å“åº”é•¿åº¦':<25} {len(report.standard_result.response):<15} {len(report.efr_result.response):<15} {report.improvement_metrics.get('response_quality_improvement', 0):+.1f}%")
        
        # æ¥æºåˆ†å¸ƒå¯¹æ¯”
        print(f"\nğŸ” æ£€ç´¢æ¥æºåˆ†å¸ƒå¯¹æ¯”:")
        print(f"{'æ¥æºç±»å‹':<20} {'æ ‡å‡†æŸ¥è¯¢':<15} {'EFRæŸ¥è¯¢':<15}")
        print("-" * 50)
        
        all_sources = set(report.standard_result.source_distribution.keys()) | set(report.efr_result.source_distribution.keys())
        for source in all_sources:
            standard_count = report.standard_result.source_distribution.get(source, 0)
            efr_count = report.efr_result.source_distribution.get(source, 0)
            print(f"{source:<20} {standard_count:<15} {efr_count:<15}")
        
        # å“åº”è´¨é‡å¯¹æ¯”
        print(f"\nğŸ’¬ å“åº”è´¨é‡å¯¹æ¯”:")
        print(f"\nğŸ“ æ ‡å‡†æŸ¥è¯¢å“åº”:")
        print(f"{report.standard_result.response[:300]}...")
        print(f"\nğŸš€ EFRå¢å¼ºå“åº”:")
        print(f"{report.efr_result.response[:300]}...")
        
        # æ”¹è¿›æ€»ç»“
        print(f"\nğŸ¯ æ”¹è¿›æ€»ç»“:")
        improvements = []
        if report.improvement_metrics.get('retrieval_count_improvement', 0) > 0:
            improvements.append(f"æ£€ç´¢æ•°é‡æå‡ {report.improvement_metrics['retrieval_count_improvement']:.1f}%")
        if report.improvement_metrics.get('score_improvement', 0) > 0:
            improvements.append(f"å¹³å‡åˆ†æ•°æå‡ {report.improvement_metrics['score_improvement']:.1f}%")
        if report.improvement_metrics.get('response_quality_improvement', 0) > 0:
            improvements.append(f"å“åº”è´¨é‡æå‡ {report.improvement_metrics['response_quality_improvement']:.1f}%")
        
        if improvements:
            print("âœ… " + ", ".join(improvements))
        else:
            print("âš ï¸ æœªå‘ç°æ˜¾è‘—æ”¹è¿›")
        
        if report.improvement_metrics.get('time_change', 0) > 0:
            print(f"â±ï¸ å¤„ç†æ—¶é—´å¢åŠ  {report.improvement_metrics['time_change']:.1f}%")
    
    async def _save_detailed_comparison(self, report: ComparisonReport):
        """ä¿å­˜è¯¦ç»†å¯¹æ¯”ç»“æœåˆ°æ–‡ä»¶"""
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path("./efr_comparison_results")
            output_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆæ–‡ä»¶å
            query_safe = "".join(c for c in report.query if c.isalnum() or c in (' ', '-', '_')).rstrip()
            query_safe = query_safe.replace(' ', '_')[:50]
            timestamp = int(time.time())
            filename = f"comparison_{query_safe}_{timestamp}.json"
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            detailed_data = {
                'timestamp': timestamp,
                'query': report.query,
                'standard_result': asdict(report.standard_result),
                'efr_result': asdict(report.efr_result),
                'improvement_metrics': report.improvement_metrics,
                'detailed_analysis': report.detailed_analysis
            }
            
            filepath = output_dir / filename
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(detailed_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ è¯¦ç»†å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¯¦ç»†ç»“æœå¤±è´¥: {e}")
    
    async def batch_comparison(
        self, 
        queries: List[str], 
        mode: str = "hybrid"
    ) -> List[ComparisonReport]:
        """æ‰¹é‡å¯¹æ¯”å¤šä¸ªæŸ¥è¯¢"""
        print(f"\nğŸ”„ å¼€å§‹æ‰¹é‡å¯¹æ¯” {len(queries)} ä¸ªæŸ¥è¯¢...")
        
        reports = []
        for i, query in enumerate(queries, 1):
            print(f"\n[{i}/{len(queries)}] å¤„ç†æŸ¥è¯¢: {query[:50]}...")
            try:
                report = await self.compare_single_query(query, mode, save_detailed=True)
                reports.append(report)
            except Exception as e:
                print(f"âŒ æŸ¥è¯¢ {i} å¤±è´¥: {e}")
                continue
        
        # ç”Ÿæˆæ‰¹é‡å¯¹æ¯”æ€»ç»“
        self._generate_batch_summary(reports)
        
        return reports
    
    def _generate_batch_summary(self, reports: List[ComparisonReport]):
        """ç”Ÿæˆæ‰¹é‡å¯¹æ¯”æ€»ç»“"""
        if not reports:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„å¯¹æ¯”ç»“æœ")
            return
        
        print(f"\nğŸ“Š æ‰¹é‡å¯¹æ¯”æ€»ç»“ ({len(reports)} ä¸ªæŸ¥è¯¢)")
        print("=" * 80)
        
        # è®¡ç®—å¹³å‡æ”¹è¿›æŒ‡æ ‡
        avg_metrics = {}
        for metric in ['retrieval_count_improvement', 'score_improvement', 'time_change', 'response_quality_improvement']:
            values = [r.improvement_metrics.get(metric, 0) for r in reports]
            avg_metrics[metric] = sum(values) / len(values)
        
        print(f"\nğŸ“ˆ å¹³å‡æ”¹è¿›æŒ‡æ ‡:")
        print(f"æ£€ç´¢æ•°é‡æ”¹è¿›: {avg_metrics['retrieval_count_improvement']:+.1f}%")
        print(f"å¹³å‡åˆ†æ•°æ”¹è¿›: {avg_metrics['score_improvement']:+.1f}%")
        print(f"å¤„ç†æ—¶é—´å˜åŒ–: {avg_metrics['time_change']:+.1f}%")
        print(f"å“åº”è´¨é‡æ”¹è¿›: {avg_metrics['response_quality_improvement']:+.1f}%")
        
        # æˆåŠŸç‡ç»Ÿè®¡
        successful_improvements = sum(1 for r in reports if any(
            r.improvement_metrics.get(metric, 0) > 0 
            for metric in ['retrieval_count_improvement', 'score_improvement', 'response_quality_improvement']
        ))
        
        print(f"\nğŸ¯ æ”¹è¿›æˆåŠŸç‡: {successful_improvements}/{len(reports)} ({successful_improvements/len(reports)*100:.1f}%)")
        
        # ä¿å­˜æ‰¹é‡æ€»ç»“
        try:
            output_dir = Path("./efr_comparison_results")
            output_dir.mkdir(exist_ok=True)
            
            summary_data = {
                'total_queries': len(reports),
                'successful_queries': len(reports),
                'average_metrics': avg_metrics,
                'improvement_success_rate': successful_improvements / len(reports),
                'individual_reports': [asdict(r) for r in reports]
            }
            
            timestamp = int(time.time())
            summary_file = output_dir / f"batch_summary_{timestamp}.json"
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ æ‰¹é‡å¯¹æ¯”æ€»ç»“å·²ä¿å­˜åˆ°: {summary_file}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ‰¹é‡æ€»ç»“å¤±è´¥: {e}")


async def setup_rag_for_comparison():
    """è®¾ç½®ç”¨äºå¯¹æ¯”çš„RAGå®ä¾‹"""
    print("ğŸš€ è®¾ç½®RAGAnythingç”¨äºEFRå¯¹æ¯”æµ‹è¯•...")
    
    # Check API configuration
    api_key = os.getenv("LLM_BINDING_API_KEY")
    base_url = os.getenv("LLM_BINDING_HOST", "https://yinli.one/v1")
    
    if not api_key:
        print("âŒ æœªæ‰¾åˆ° API å¯†é’¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ LLM_BINDING_API_KEY")
        return None
    
    # Create EFR configuration
    efr_config = EFRConfig(
        rrf_k=60,
        rrf_weights={
            "vector": 1.0,
            "entity": 0.8,
            "relation": 0.7,
            "chunk": 0.9,
        },
        enable_rerank=True,
        rerank_top_k=20,
        min_rerank_score=0.3,
        enable_mmr=True,
        mmr_lambda=0.7,
        mmr_top_k=10,
        enable_recency=True,
        recency_weight=0.2,
        enable_source_trust=True,
        source_trust_weights={
            "academic": 1.0,
            "official": 0.9,
            "news": 0.7,
            "blog": 0.5,
            "unknown": 0.6,
        },
    )
    
    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./efr_comparison_storage",
        parser="mineru",
        parse_method="auto",
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )
    
    # Define model functions
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    
    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )
    
    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        efr_config=efr_config,
        enable_efr=True,
        llm_model_func=llm_model_func,
        embedding_func=embedding_func,
    )
    
    return rag


async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºEFRå¯¹æ¯”åŠŸèƒ½"""
    print("ğŸŒŸ RAGAnything EFR Layer æ•ˆæœå¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # è®¾ç½®RAG
    rag = await setup_rag_for_comparison()
    if not rag:
        return
    
    # å¤„ç†ç¤ºä¾‹æ–‡æ¡£
    sample_doc = project_root / "example_doc" / "2005.11401v4.pdf"
    if sample_doc.exists():
        print(f"\nğŸ“„ å¤„ç†ç¤ºä¾‹æ–‡æ¡£: {sample_doc.name}")
        try:
            await rag.process_document_complete(
                file_path=str(sample_doc),
                output_dir="./efr_comparison_output",
                parse_method="auto"
            )
            print("âœ… æ–‡æ¡£å¤„ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
    
    # åˆ›å»ºå¯¹æ¯”æµ‹è¯•å™¨
    comparator = EFRComparison(rag)
    
    # æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
    test_queries = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ çš„ä¸»è¦åº”ç”¨é¢†åŸŸï¼Ÿ",
        "æ·±åº¦å­¦ä¹ ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸæœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
        "ç¥ç»ç½‘ç»œçš„åŸºæœ¬åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æ•°æ®æŒ–æ˜çš„ä¸»è¦æŠ€æœ¯æœ‰å“ªäº›ï¼Ÿ",
    ]
    
    print(f"\nğŸ” å¼€å§‹å¯¹æ¯”æµ‹è¯•...")
    
    # é€‰æ‹©æµ‹è¯•æ¨¡å¼
    print("\nè¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. å•ä¸ªæŸ¥è¯¢è¯¦ç»†å¯¹æ¯”")
    print("2. æ‰¹é‡æŸ¥è¯¢å¯¹æ¯”")
    print("3. è‡ªå®šä¹‰æŸ¥è¯¢å¯¹æ¯”")
    
    try:
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
        
        if choice == "1":
            # å•ä¸ªæŸ¥è¯¢è¯¦ç»†å¯¹æ¯”
            query = test_queries[0]
            await comparator.compare_single_query(query, mode="hybrid", save_detailed=True)
            
        elif choice == "2":
            # æ‰¹é‡æŸ¥è¯¢å¯¹æ¯”
            await comparator.batch_comparison(test_queries, mode="hybrid")
            
        elif choice == "3":
            # è‡ªå®šä¹‰æŸ¥è¯¢å¯¹æ¯”
            custom_query = input("\nè¯·è¾“å…¥ä½ çš„æŸ¥è¯¢: ").strip()
            if custom_query:
                await comparator.compare_single_query(custom_query, mode="hybrid", save_detailed=True)
            else:
                print("âŒ æœªè¾“å…¥æŸ¥è¯¢")
                
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
    
    print("\nğŸ‰ EFRå¯¹æ¯”æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: ./efr_comparison_results/")
    print("ğŸ“Š æŸ¥çœ‹è¯¦ç»†å¯¹æ¯”æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯")


if __name__ == "__main__":
    asyncio.run(main())

